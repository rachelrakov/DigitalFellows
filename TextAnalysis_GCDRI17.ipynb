{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCDRI Jan 2017:  Text Analysis and Text Classification with NLTK and scikit-learn\n",
    "- Jupyter Notebook by Rachel Rakov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!  Let's get started by importing some data!\n",
    "- A large collection of textual data is called a *corpus* (pluralized as *corpora*) \n",
    "- I will be using the term corpus or corpora throughout this workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's take a look at some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print text3[:100]\n",
    "#print (text3[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "#### Shows the context of a particular word in your corpus, with a default width of 40 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"death\"\n",
    "#width = int\n",
    "text1.concordance(word)\n",
    "\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text3.concordance(word)\n",
    "print \n",
    "\n",
    "grail_death = text6.concordance(word)\n",
    "print grail_death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common contexts\n",
    "### Takes two words as an argument, returns contexts in which they appear similarly across the text (within one text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.common_contexts([\"pretty\", \"very\"])\n",
    "#requires a list as an argument\n",
    "## across the same text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion plots\n",
    "### Used to see where particular words appear in your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#text6.dispersion_plot([\"knights\", 'ARTHUR', 'ROBIN', 'grail', 'lady',\"ni\"])\n",
    "#text1.dispersion_plot([\"Starbuck\", 'whale', 'Ahab', 'Ishmael', 'sea',\"death\"])\n",
    "text3.dispersion_plot([\"God\", \"apple\", \"garden\", \"woman\"])\n",
    "\n",
    "##Shows location of word in a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great!  Let's count things now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many words are in the texts?\n",
    "print len(text1)\n",
    "print len(text6)\n",
    "print \"\\n\"\n",
    "\n",
    "\n",
    "# How many times are particular words in the text?\n",
    "print text1.count(\"Ahab\")\n",
    "print text6.count(\"Arthur\")\n",
    "print text6.count(\"ARTHUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How can I get percentages of words in a text?\n",
    "print 100* text1.count(\"the\")/float(len(text1))\n",
    "\n",
    "print 100* text3.count(\"the\")/float(len(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "fdist = FreqDist(text1)\n",
    "#print fdist\n",
    "\n",
    "#fdist.most_common(100)\n",
    "\n",
    "#fdist[\"whale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## shows how many times \"whale\" occurs in text\n",
    "fdist[\"whale\"] \n",
    "\n",
    "#shows percentage of \"whale\" in the text\n",
    "fdist.freq(\"whale\")\n",
    "\n",
    "# shows the most common token\n",
    "#fdist.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A happy chart from \"Natural Language Processing with Python\" (Bird, Klein & Loper)\n",
    "       # Example                                        #Description\n",
    "      \n",
    "    fdist = FreqDist(samples) \t   create a frequency distribution containing the given samples\n",
    "    fdist[sample] += 1 \t           increment the count for this sample\n",
    "    fdist['monstrous'] \t           count of the number of times a given sample occurred\n",
    "    fdist.freq('monstrous') \t   frequency of a given sample\n",
    "    fdist.N() \t                   total number of samples\n",
    "    fdist.most_common(n) \t       the n most common samples and their frequencies\n",
    "    for sample in fdist:           iterate over the samples\n",
    "    fdist.max() \t               sample with the greatest count\n",
    "    fdist.tabulate() \t           tabulate the frequency distribution\n",
    "    fdist.plot() \t               graphical plot of the frequency distribution\n",
    "    fdist.plot(cumulative=True) \tumulative plot of the frequency distribution\n",
    "    fdist1 |= fdist2 \t           update fdist1 with counts from fdist2\n",
    "    fdist1 < fdist2 \t test if samples in fdist1 occur less frequently than in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = \"Far out in the uncharted backwaters of the unfashionable end of the Western Spiral arm of the Galaxy lies\"\\\n",
    "\" a small unregarded yellow sun.  Orbiting this at a distance at roughly nintey-eight million miles is an utterly \"\\\n",
    "\"insignificant little blue-green planet whose ape-descended life forms are so amazingly primitive that they still think \"\\\n",
    "\"digital watches are a pretty neat idea.\"\n",
    "print paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = word_tokenize(paragraph)\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text comparisions using Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = brown.categories()\n",
    "for i in cats:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "romance_sent = brown.sents(categories=[\"romance\"])\n",
    "print romance_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = brown.words(categories=[\"news\"])  #get all of the words from the \"news\" category\n",
    "romance = brown.words(categories=[\"romance\"]) # get all of the words from the \"romance\" category\n",
    "\n",
    "## Build some frequency distribution!!!! \n",
    "fdist_news = FreqDist(w.lower() for w in news)\n",
    "fdist_romance = FreqDist(w.lower() for w in romance)\n",
    "\n",
    "modals = [\"can\", \"could\", \"might\", \"may\", \"would\", \"must\", \"will\"]\n",
    "\n",
    "print \"word:\\t news \\t \\t romance\"\n",
    "print \"_________________________________\"\n",
    "for m in modals:\n",
    "    print m +\":\",\"\\t\", \"%f \\t %f\"  %(fdist_news.freq(m)*100, fdist_romance.freq(m)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag  ###part of speech tags a list\n",
    "from nltk import pos_tag_sents ### pos tags sentences, rather than individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = \"Far out in the uncharted backwaters of the unfashionable end of the Western Spiral arm of the Galaxy lies\"\\\n",
    "\" a small unregarded yellow sun.  Orbiting this at a distance at roughly nintey-eight million miles is an utterly \"\\\n",
    "\"insignificant little blue-green planet whose ape-descended life forms are so amazingly primitive that they still think \"\\\n",
    "\"digital watches are a pretty neat idea.\"\n",
    "p = word_tokenize(paragraph)\n",
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph_POS = pos_tag(p)\n",
    "print paragraph_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Because not all of these tags are intuitive ###\n",
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using the Brown Corpus\n",
    "#### Can we train a computer to predict whether a sentence belongs in the news corpus or the romance corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech - number of nouns in sentences\n",
    "Let's start by getting all of the sentences from the news and romance categories of the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_sent = brown.sents(categories=[\"news\"])\n",
    "romance_sent = brown.sents(categories=[\"romance\"])\n",
    "\n",
    "print len(news_sent)\n",
    "print len(romance_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's part of speech tag each word in the sentence!\n",
    "\n",
    "Note: We use pos_tag_sent because we are tagging sentences, rather than individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_pos = pos_tag_sents(news_sent)\n",
    "romance_pos = pos_tag_sents(romance_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that will count how many nouns are in each sentence of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countNouns(pos_tag_sents):\n",
    "    noun_count = 0\n",
    "    all_noun_counts = []\n",
    "    for sentence in pos_tag_sents:\n",
    "        for word in sentence:\n",
    "            tag = word[1]\n",
    "            if tag [:2] == \"NN\":  ## so that we capture both singluar and plural nouns\n",
    "                noun_count = noun_count+1\n",
    "        all_noun_counts.append(noun_count)\n",
    "        noun_count = 0\n",
    "    return all_noun_counts\n",
    "\n",
    "news_counts = countNouns(news_pos)\n",
    "romance_counts = countNouns(romance_pos)\n",
    "\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print romance_counts[:20]\n",
    "print news_counts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning:  Building train and test sets\n",
    "### Seperating data, getting labels, and aligning them with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = [\"news\", \"romance\"] #define what categories of brown corpus we want\n",
    "print cats    \n",
    "text = [brown.sents(categories=cat) for cat in cats]\n",
    "test_sets_int = 500 ## specify how many test sentences we will have per category\n",
    "\n",
    "######## create labels for test and training sets ##############\n",
    "### find how many sentences there are, subtract test_sets_int for the correct number of training and testing labels\n",
    "lengths = []\n",
    "for i in range(len(cats)):\n",
    "    start_length = len(text[i])\n",
    "    print start_length\n",
    "    length = start_length - test_sets_int\n",
    "    #print length\n",
    "    lengths.append(length)\n",
    "\n",
    "print lengths\n",
    "\n",
    "#### concatenate the labels together #############\n",
    "train_labels = [\"news\"]*lengths[0]+[\"romance\"]*lengths[1]\n",
    "test_labels = [\"news\"]*test_sets_int+[\"romance\"]*test_sets_int\n",
    "\n",
    "print train_labels[:10]\n",
    "print train_labels[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's separate out the training data from the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### take the first 500 count features from each dataset and use them as test - use the rest as train ######\n",
    "news_values_test = news_counts[:test_sets_int]\n",
    "news_values_train = news_counts[test_sets_int:]\n",
    "romance_values_test = romance_counts[:test_sets_int]\n",
    "romance_values_train = romance_counts[test_sets_int:]\n",
    "print len(news_values_test)\n",
    "print len(news_values_train)\n",
    "print len (romance_values_test)\n",
    "print len(romance_values_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### concatenate the lists of data together ######\n",
    "train_features = news_values_train+romance_values_train\n",
    "test_features = news_values_test+romance_values_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to pandas DataFrames\n",
    "So new we have both train_features and train_labels, as well as test_features and test_labels.  Let's manage this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### create two DataFrames - one for train, one for testing ####\n",
    "train_data = pd.DataFrame(train_features, columns=[\"number of nouns\"])\n",
    "test_data = pd.DataFrame(test_features, columns=[\"number of nouns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add our labels to our dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### you can add columns to DataFrames like this! ####\n",
    "train_data[\"labels\"] = train_labels\n",
    "test_data[\"labels\"] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification using scikit learn\n",
    "\n",
    "#### Seperate the dataframe into data and labels, for both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### We use the naming conventions of sklearn here ########\n",
    "X_train = train_data[\"number of nouns\"]\n",
    "y_train = train_data[\"labels\"]\n",
    "\n",
    "\n",
    "X_test = test_data[\"number of nouns\"]\n",
    "y_test = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that because we only have one feature, we need to reshape our data\n",
    "\n",
    "##### sklearn will tell you when your data needs to be reshaped, and will tell you how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train_features)\n",
    "print len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Import your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an instance of your classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit, predict, and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    print predictions[i], y_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra materials\n",
    "We may not have time to get to these things, but here are some additional materials to help you in your classification future!\n",
    "- Changing classifiers\n",
    "- Adding additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with k-nearest neighbors\n",
    "What if we used the classifier we described before, k-nearest neighbors?  How well do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember our steps from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  ### import your classifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)  ### Create a new instance of your classifier \n",
    "\n",
    "classifier.fit(X_train, y_train)  ### fit\n",
    "\n",
    "predictions = classifier.predict(X_test) ### predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test) ### score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions) ### evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add another feature - number of modal verbs in a sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### If any of these modal words appear in our sentences, accumulate the total for each sentence\n",
    "\n",
    "def modals(setType):\n",
    "    modals_count = 0\n",
    "    modal_features = []\n",
    "    modals = [\"can\", \"could\", \"might\", \"may\", \"would\", \"must\", \"will\"]\n",
    "    for sent in setType:\n",
    "        for word in modals:\n",
    "            if word in sent:\n",
    "                modals_count = modals_count+1\n",
    "        modal_features.append(modals_count)\n",
    "        modals_count = 0\n",
    "    print len(modal_features)   \n",
    "    return modal_features\n",
    "\n",
    "news_modals = modals((brown.sents(categories=\"news\")))\n",
    "romance_modals = modals((brown.sents(categories=\"romance\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print news_modals[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test sets of modal features\n",
    "Second verse, same as the first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### create feature vectors of modal counts #####\n",
    "news_modals_test = news_modals[:test_sets_int]\n",
    "news_modals_train = news_modals[test_sets_int:]\n",
    "romance_modals_test = romance_modals[:test_sets_int]\n",
    "romance_modals_train = romance_modals[test_sets_int:]\n",
    "print len(news_modals_test)\n",
    "print len(news_modals_train)\n",
    "print len (romance_modals_test)\n",
    "print len(romance_modals_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### concatenate the modal features #####\n",
    "modal_features_train = news_modals_train+romance_modals_train\n",
    "modal_features_test = news_modals_test+romance_modals_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print modal_features_train[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns to existing DataFrames\n",
    "You can add columns in DataFrames by location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.insert(1, \"number of modals\", modal_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.insert(1, \"number of modals\", modal_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting DataFrames with more than one feature\n",
    "Split columns based on column order, or use the name of the column to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_data[train_data.columns[:2]]\n",
    "y_train = train_data[\"labels\"]\n",
    "\n",
    "\n",
    "X_test = test_data[test_data.columns[:2]]\n",
    "y_test = test_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:  Because we have more than one feature, we no longer need to reshape our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify like before!  (No need to re-import your classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train,y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't actually improve things by much, but that also shouldn't be too much of a surprise, from when we looked at it before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
